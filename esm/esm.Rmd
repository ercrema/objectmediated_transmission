---
title: "How cultural transmission through objects impacts inferences about cultural evolution: supplementary material"
author: "Crema,E.R., Bortolini, E., Lake,M.W."
date: "20 Dec 2021"
output: html_document
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, settings_libraries_functions, include=FALSE}
# Load Required Packages
library(foreach)
library(doParallel)
library(RColorBrewer)
library(here)

#Source Functions
source(here('src.R'))
```



# Models

Consider a population of $N$ individuals each associated with a mental template of a cultural variant. At each generation step, each individual produce $n$ copies of a physical realisation of the mental template, with $n$ drawn from a Poisson distribution with intensity $lambda$. After this _production event_ all individuals update their mental template by randomly selecting an object and copying the associated cultural variant. New variants are introduced into the population in two distinct ways:

## Decoding Error

Under this scenario, with probability $\mu_{d}$, the focal individual fails to correctly reconstruct the mental template associated to the object it copies from, and as consequence introduces a new cultural variant in the population of the mental templates. The new variants are introduced following the infinite allele model (i.e. there are 0 probability of convergent error).  
<!--Is infinite allele realistic here? errors are likely to have some dependency to the target mental template. Perhaps a random step error model is more appropriate? Worth discussing this a least? -->

## Encoding Error

With probability $\mu_{e}$ an object produced by the focal individual is no longer directly asociated to its parent mental template. As for the decoding error scenario described above new variants are introduced following the infinite allele model. Howeverthe mental template of the focal individual remains unchanged (unless it subsequently select the mutant object in the transmission process) and an individual could produce more than one mutant object, each associated with a different mental template that does not exist in the population of the producers yet.  

## Model Scheduling and Data Collection

Both models are initiated with $N$ individuals associated with different mental template represented by an integer value. At each time-step the following processes occur:

1. *Production*. For each agent $i$, $n_i$ identical objects with the same numerical value as the mental template of the agent are created, with $n_i \sim Poisson(\lambda)$.

2. *Encoding Error* With probability $\mu_{e}$, each object value is associated with a new integer value that has not be introduced in the simulation run. 

3. *Cultural Transmission*. Each agent randomly selects an object and updates its mental template number with the integer value associated with the object.

4. *Decoding Error* With probability $\mu_{d}$, each agent updates its mental template value with a new number that has not been previosuly used in the simulation run. This portrays an instance where the individual fails to correctly reconstuct the mental template of an object.

Models are executed first for $T_{burnin}+T_{collection}$ timesteps. The first tem ensures that the simulation reach equilibrium conditions. During $T_{collection}$, the frequency of different cultural variants associated to the objects are retained for analysis. 

Notice that both $mu_e$ and $mu_d$ can be positive and two errors can co-exist. However in this case will explore each error type separately, referring to instances of $\mu_e>0,\mu_d=0$ as _encoding error model_ and $\mu_e=0,\mu_d>0$ as _decoding error model_.

### Wright Fisher Model

For comparative purposes we also simulated the outputs of a Wright-Fisher model where individuals directly copy the mental template and with a probability $\mu$ in the copying process.

# Experiment Design

We compared the output of our three scenarios (wright-fisher, unbiased object-mediated transmission with encoding error, and unbiased object-mediated transmission with decoding error) using different descriptive statistics and visualisation techniques (diversity, richness, turnover rate, and progeny distribution) employed in cultural evolutionary studies.

## Richness and Diversity

We computed the richness $k$ (the number of unique variants) and the diversity $D$ of the distribution of cultural variants in the final set of objects produced by our population. Diversity is calculated using Simpson's Index, equivalent to $1-\sum_{i}^{k}p_{i}^2$, with $p_i$ being the propotion of the $i$-th variant in the population of objects.

### Model parameters

We use the following parameters:

```{r, parameters_sumstat}
nsim = 1000 #number of repetitions for each parameter combination
n.objects = 1000 #target number of objects produced at each timestep 
N <- c(50,500,1000,2000) #number of individuals (producers)
lambda <- n.objects/N # production rate
# create parameter spaces
pspace = data.frame(N=rep(N,each=nsim), lambda=rep(lambda,each=nsim)) 
mu.e = mu.d = mu = 0.01 #error rates
timesteps <- 5000 #number of timesteps
```

We explored four different parameter combinations of $\lambda$ and $N$ ensuring that their product were hold constant and approximately equal to `r n.objects`. For each parameter combination we executed `r nsim` runs, using a mutation rate of `r mu` (notice that `mu` in this case refers intechangebly to $\mu$,$\mu_{e}$, and $\mu_{d}$). We then computed the richness $k$ and diversity $D$ using the frequency distribution of variants (objects) produced in the final timestep. We run each simulation for `r timesteps` timesteps, which ensured that that model reached equilibrium conditions and hence unaffected by its initial conditions.  

```{r,register_clusters_sumstat,include=FALSE}
#Setup parallel processing using n-1 threads, with n being maximum possible for the specific machine
cl <-  makeCluster(detectCores()-1)  
registerDoParallel(cl)
```

```{r execution_sumstat}
res.wf = foreach (i = 1:nsim, .combine='cbind') %dopar% {
  unlist(wf(N=n.objects,mu=mu,timesteps = 5000,output="sumstat"))
}
res.wf.div = res.wf[1,]
res.wf.k = res.wf[2,]


res.decode = foreach (i = 1:nrow(pspace), .combine='cbind') %dopar% {
  unlist(objTr(N=pspace$N[i],lambda=pspace$lambda[i],mu.e=0,mu.d=mu.d,timesteps = 5000,output="sumstat"))}
res.decode.div=matrix(res.decode[1,],ncol=length(N),nrow=nsim)
res.decode.k=matrix(res.decode[2,],ncol=length(N),nrow=nsim)

res.encode = foreach (i = 1:nrow(pspace), .combine='cbind') %dopar% {
  unlist(objTr(N=pspace$N[i],lambda=pspace$lambda[i],mu.e=mu.e,mu.d=0,timesteps = 5000,output="sumstat"))}
res.encode.div=matrix(res.encode[1,],ncol=length(N),nrow=nsim)
res.encode.k=matrix(res.encode[2,],ncol=length(N),nrow=nsim)

```

```{r, close_clusters_sumstat, include=FALSE}
#stop cluster
stopCluster(cl)
```


```{r,plot_sumstat, results='hide',fig.width= 9, fig.height= 9}

par(mfrow=c(2,1))
xs=jitter(rep(c(1,4,7,10,2,5,8,11,14),each=nsim),factor=1.2)

# Diversity
ys.div=(c(res.decode.div,res.encode.div,res.wf.div))

lo.div=quantile(res.wf.div,0.025)
hi.div=quantile(res.wf.div,0.975)
col.div=vector(length=length(ys.div))

col.div[ys.div>=lo.div&ys.div<=hi.div]=rgb(0,0,0,0.05)
col.div[ys.div<=lo.div]=add.alpha("royalblue",0.05)
col.div[ys.div>=hi.div]=add.alpha("indianred",0.05)

plot(xs,ys.div,col=col.div,pch=20,axes=F,xlab="",ylab="Diversity")
abline(h=c(lo.div,hi.div),col=c("royalblue","indianred"),lty=4)
axis(side=1,at=c(1.5,4.5,7.5,10.5,14),labels=c(N,n.objects))
mtext(side=1,line=2.5,"N")
axis(side=2)
axis(side=3,at=c(1.5,4.5,7.5,10.5,14),labels=c(n.objects/N,"NA\n (Wright-Fisher)"))
mtext(side=3,line=2.5,expression(lambda))
box()

text(4,0.2,"Decoding Error",srt=90,cex=0.8)
text(5,0.2,"Enccoding Error",srt=90,cex=0.8)

# Richness
ys.k=(c(res.decode.k,res.encode.k,res.wf.k))

lo.k=quantile(res.wf.k,0.025)
hi.k=quantile(res.wf.k,0.975)
col.k=vector(length=length(ys.k))

col.k[ys.k>=lo.k&ys.k<=hi.k]=rgb(0,0,0,0.05)
col.k[ys.k<=lo.k]=add.alpha("royalblue",0.05)
col.k[ys.k>=hi.k]=add.alpha("indianred",0.05)

plot(xs,ys.k,col=col.k,pch=20,axes=F,xlab="",ylab="Richness (k)")
abline(h=c(lo.k,hi.k),col=c("royalblue","indianred"),lty=4)
axis(side=1,at=c(1.5,4.5,7.5,10.5,14),labels=c(N,n.objects))
mtext(side=1,line=2.5,"N")
axis(side=2)
axis(side=3,at=c(1.5,4.5,7.5,10.5,14),labels=c(n.objects/N,"NA\n (Wright-Fisher)"))
mtext(side=3,line=2.5,expression(lambda))
box()

text(1,44,"Decoding Error",srt=90,cex=0.8)
text(2,44,"Enccoding Error",srt=90,cex=0.8)
```

Both richness and diversity showed consistently lower values for both versions of object-mediated transmission, even in situations where the $N$ is larger than the value set for the Wright-Fisher model. With lower values of $N$ (and consequently in this case higher values of $\lambda$) the number of mental templates is limited and hence there is constraint in the ammount of variability in the system. For example under the decoding error scenario with $N=$ `r N[1]` the highest richness values possible (assuming all individuals have a different mental template) is `r N[1]` ($k=N$), whilst the highest diversity value is `r 1-(1/N[1])^2` ($1-\left(\frac{1}{N}\right)^{2}N$). Under the encoding error scenario the values are higher for richness (limited approximately to $N+\lambda N\mu_{e}$, in this case `r round(N[1] + lambda[1]*N[1]*mu.e)`) and just slighlty higher for diversity, capped aproximately at $1 - \left( \frac{(1-\mu_e)}{N} \right)^2N+\left(\frac{1}{\lambda N}\right)^2\lambda N \mu_e$ (equal to `r 1-N[1]*((1-mu.e)/N[1])^2+(1/(lambda[1]*N[1]))^2*lambda[1]*N[1]*mu.e`). In both cases the difference is dictated by $\mu_e$ (higher values lead to higher diversity) and $\lambda$ higher values lead to lower diversity. These differences (or lack thereof) can be observed when we compare the two forms of error for each parameter combination of objected-mediated transmission. 
The lower diversity and richness of the object-mediated transmission is however not just explained by the combination of low $N$ and high $\lambda$. In two of the parameter combinations we considered instances where the $N$ was equal or higher for the object mediated transmission, yet both diversity and richness yielded lower figures. This is explained by the additional drift caused by stochasticity in the production event. For example with $\lambda=1$, approximately `r round(dpois(0,1)*100,2)`% of the individuals will not produce an object, limiting the ammount of richness and diversity that could be potentially realised.

## Turnover Rate

Bentley et al 2007[1](https://doi.org/10.1016/j.evolhumbehav.2006.10.002) defined he turnover rate $z$ as the average number of new entires in the list of the top $y$ cultral variants ordered by their absolute frequencies. In particular they noticed that under neutrality $y$ and $z$ have a linear relationship $z=y\sqrt{\mu}$. A subsequent study by Evans and Giometto [2](https://arxiv.org/abs/1105.4044v1) has shown that this relationship depends also on $N$. When $N\mu < 0.15y$ this was approximately $z=2\cdot N \mu$, and when $N\mu > 0.15y$, $z=d\cdot \mu^{a} y^{b} N^{c}$, with the constants $a=0.55$, $b=0.86$, $c=0.13$ and $d=0.138$. Acerbi and Bentley [3](http://dx.doi.org/10.1016/j.evolhumbehav.2014.02.003) have used simplified this relationship to $z=Ay^b$, so that that $b$ can be estimated from the observed turnover rates, with the expectation that under neutrality this is equivalent to 0.86 in the regions where $N\mu > 0.15y$. 

Here we examine whether and how, under an object-mediated transmission regime, estimates of $b$ differ from those expected under neutrality. As for in the first expriment above we explore scenarios with the same number of expected objects under different combinations of $N$ and $\lambda$: 

```{r, parameters_turnover}
nsim = 100 #number of repetitions for each parameter combination
n.objects = 1000 #target number of objects produced at each timestep 
N <- c(50,500,1000,2000) #number of individuals (producers)
lambda <- n.objects/N # production rate
# create parameter spaces
pspace = data.frame(N=rep(N,each=nsim), lambda=rep(lambda,each=nsim)) 
mu.e = mu.d = mu = 0.01 #error rates
timesteps <- 5500 #number of timesteps
warmup <- 5000
top <- 10
```

```{r,register_clusters_turnover, include=FALSE}
#Setup parallel processing using n-1 threads, with n being maximum possible for the specific machine
cl <-  makeCluster(detectCores()-1)  
registerDoParallel(cl)
```


```{r, execution_turnover}
wf.tr = foreach (i=1:nsim) %dopar% {
  tmp=wf(N = n.objects,mu=mu,timesteps=5501,warmup=5000,output="turnover",verbose=FALSE,top=top)
  list(tmp$z.frame[,2],tmp$b)
}

encode.tr = foreach(i=1:nrow(pspace)) %dopar% {
  tmp=objTr(N = pspace$N[i],lambda=pspace$lambda[i],mu.e=mu.e,mu.d=0,timesteps=5501,warmup=5000,output="turnover",verbose=FALSE,top=top)
  list(tmp$z.frame[,2],tmp$bN,tmp$bNp)  
}

decode.tr = foreach(i=1:nrow(pspace)) %dopar% {
  tmp=objTr(N = pspace$N[i],lambda=pspace$lambda[i],mu.e=0,mu.d=mu.d,timesteps=5501,warmup=5000,output="turnover",verbose=FALSE,top=top)
  list(tmp$z.frame[,2],tmp$bN,tmp$bNp)  
}

```

```{r,stop_cluster_turnover, include=FALSE}
#stop cluster
stopCluster(cl)
```


```{r, post_processing_turnover}
wf.rates=matrix(unlist(lapply(wf.tr,'[[',1)),nrow=10)
wf.b=unlist(lapply(wf.tr,'[[',2))

encode.rates.mat=matrix(unlist(lapply(encode.tr,'[[',1)),nrow=top)
encode.rates = vector("list",length=length(N))
encode.b=matrix(unlist(lapply(encode.tr,'[[',2)),nrow=nsim)

decode.rates.mat=matrix(unlist(lapply(decode.tr,'[[',1)),nrow=top)
decode.rates = vector("list",length=length(N))
decode.b=matrix(unlist(lapply(decode.tr,'[[',2)),nrow=nsim)

legItems.turnover=vector(length=length(lambda))


for (i in 1:length(N))
{
  encode.rates[[i]] = encode.rates.mat[,(i*nsim-(nsim-1)):(i*nsim)]
  decode.rates[[i]] = decode.rates.mat[,(i*nsim-(nsim-1)):(i*nsim)]
  legItems.turnover[i] = as.expression(bquote(paste(lambda,"=",.(lambda[i]),"; N=",.(N[i]),sep="")))
}

legItems.turnover = c(legItems.turnover,paste0("Wright-Fisher (N=",n.objects,")"))


# Compute Means and 95% percentiles
encode.b.hi=apply(encode.b,2,quantile,0.975)
encode.b.mean=apply(encode.b,2,mean)
encode.b.lo=apply(encode.b,2,quantile,0.025)

decode.b.hi=apply(decode.b,2,quantile,0.975)
decode.b.mean=apply(decode.b,2,mean)
decode.b.lo=apply(decode.b,2,quantile,0.025)

wf.b.hi=quantile(wf.b,0.975)
wf.b.mean=mean(wf.b)
wf.b.lo=quantile(wf.b,0.025)
```

Estimates of $b$ deviate strongly from the neutral expectation only under an extreme low population / high producitivty regime. With decreasing productivity rates and $N$ value comparable to those observed for the Wright-Fisher model the difference becomes much smaller. 


```{r, plot_turnover_b_estimates, fig.width= 7, fig.height= 5}
## Estimates of b
plot(0,0,type="n",ylim=range(c(wf.b,encode.b,decode.b)),xlim=c(0.5,14.5),axes=F,xlab="",ylab="")
abline(h=0.86,lty=4)

axis(side=1,at=c(1.5,4.5,7.5,10.5,14),labels=c(N,n.objects))
mtext(side=1,line=2.5,"N")
axis(side=2)
axis(side=3,at=c(1.5,4.5,7.5,10.5,14),labels=c(n.objects/N,"NA\n (Wright-Fisher)"))
mtext(side=3,line=2.5,expression(lambda))
box()     
arrows(x0=c(1,4,7,10),x1=c(1,4,7,10),y0=encode.b.lo,y1=encode.b.hi,lty=1,lwd=2,length=0,col="indianred")
points(c(1,4,7,10),encode.b.mean,pch=20,col="indianred",cex=1.5)

arrows(x0=c(2,5,8,11),x1=c(2,5,8,11),y0=decode.b.lo,y1=decode.b.hi,lty=1,lwd=2,length=0,col="royalblue")
points(c(2,5,8,11),decode.b.mean,pch=20,col="royalblue",cex=1.5)

arrows(x0=14,x1=14,y0=wf.b.lo,y1=wf.b.hi,lwd=2,length=0)
points(14,wf.b.mean,pch=20,cex=1.5)

legend("topright",legend=c("Object-mediated with encoding error","Object-mediated with decoding error","Wright-Fisher"),lty=c(1),pch=20,col=c("indianred","royalblue","black"),cex=0.7)
```



```{r, plot_turnover_profile,fig.width=9,fig.height=5}
par(mfrow=c(1,2))
## Turnover rate plot
plot(1:10,apply(wf.rates,1,mean),type="b",xlab="Top",ylab="Turnover Rate",pch=20,ylim=range(c(wf.rates,unlist(encode.rates),na.rm=TRUE)),main="Encoding Error")
arrows(x0=1:10,x1=1:10,y0=apply(wf.rates,1,quantile,0.025),y1=apply(wf.rates,1,quantile,0.975),length=0)

cc <- brewer.pal(length(N),"Set1") 
for (i in 1:length(N))
{
lines(1:10,apply(encode.rates[[i]],1,mean),type="b",pch=20,col=cc[i])  
arrows(x0=1:10,x1=1:10,y0=apply(encode.rates[[i]],1,quantile,0.025),y1=apply(encode.rates[[i]],1,quantile,0.975),length=0,col=cc[i])
}

legend("topleft",legend=legItems.turnover,col=c(cc,"black"),pch=20,lty=1,cex=0.8,bty="n")



plot(1:10,apply(wf.rates,1,mean),type="b",xlab="Top",ylab="Turnover Rate",pch=20,ylim=range(c(wf.rates,unlist(decode.rates)),na.rm=TRUE),main="Decoding Error")
arrows(x0=1:10,x1=1:10,y0=apply(wf.rates,1,quantile,0.025),y1=apply(wf.rates,1,quantile,0.975),length=0)

cc <- brewer.pal(length(N),"Set1") 
for (i in 1:length(N))
{
lines(1:10,apply(decode.rates[[i]],1,mean),type="b",pch=20,col=cc[i])  
arrows(x0=1:10,x1=1:10,y0=apply(decode.rates[[i]],1,quantile,0.025),y1=apply(decode.rates[[i]],1,quantile,0.975),length=0,col=cc[i])
}
```

When $y$ and $z$ are plotted against each other these impressions based on the estimates of $b$ are confirmed. Both ecoding and decoding error strongly deviates from the theorethical expectations at the lowest setting of $N$ (and hence highest setting of $\lambda$). In the case of encoding error the average turn-over rate grows at higher rate with lower ranks, indicating a higher volatility determined most likely by the combination of low richness and and hence high possibility of single mutation to enter in the top-10 variants. In the case of decoding error this effect is minimised and the overall profile show a flat and less variability in the turn-over rate as a consequence of low richness. With $N=500$, the turn-over profile closely resembles the Wright-Fisher model, with just slightly higher estimates of $b$ compared to the theorethical expectations for both scenarios. With $N\geq1000$ both the estimate and the profile are indistinguishable from Wright-Fisher.



## Progeny Distribution
Bentley et al [4](https://doi.org/10.1098/rspb.2004.2746) first noted that given a temporal interval $T$, $log(P_k$), the log frequencies of variants appearing $k$ times, follow a power-law distribution under neutrality. It follows that the number of variants occuring once (i.e. $k=1$) within $T$ is far greater than those appearing twice ($k=2$), three times ($k=3$),etc., and that this rate of decline with increasing $k$ can be predicted. O'Dwyler and Kandler[5](https://doi.org/10.1098/rstb.2016.0426) recently examined the shape of such *progeny distribution* and noticed that:

* there is indeed a power law distribution, but with a constant exponent of $-3/2$, which *does not* depend on $N$ and $\mu$;
* the power law is actually followed by an exponential cut-off;
* the cut-off value depends on the innovation rate $\mu$;

This can be demonstrated by the following simulation:


```{r,execution_progeny_wf}
# changing population size
wf.prog.n300.m01 = wf(N=300,mu=0.01,warmup=10000,timesteps=20000,output="progeny")
wf.prog.n1000.m01 = wf(N=1000,mu=0.01,warmup=10000,timesteps=20000,output="progeny")
wf.prog.n3000.m01 = wf(N=3000,mu=0.01,warmup=10000,timesteps=20000,output="progeny")

# changing innovation rate
wf.prog.n1000.m005 = wf(N=300,mu=0.005,warmup=10000,timesteps=20000,output="progeny")
wf.prog.n3000.m001 = wf(N=300,mu=0.001,warmup=10000,timesteps=20000,output="progeny")

```


```{r, plot_progeny_wf, results='hide',fig.width= 9, fig.height= 5}
par(mfrow=c(1,2))
plot(wf.prog.n300.m01$d2,pch=20,log="xy",ylab="Probability P(k) of number variants >= k",xlab="k",col="darkgrey",type="b")
lines(wf.prog.n1000.m01$d2,pch=2,col="indianred",type="b")
lines(wf.prog.n3000.m01$d2,pch=3,col="royalblue",type="b")
legend("bottomleft",legend=c(expression(paste("N=300; ",mu,"=0.01")),
                             expression(paste("N=1000; ",mu,"=0.01")),
                             expression(paste("N=3000; ",mu,"=0.01")))
       ,pch=c(20,2,3),col=c("darkgrey","indianred","royalblue"),bty="n")


plot(wf.prog.n300.m01$d2,pch=20,log="xy",ylab="Probability P(k) of number variants >= k",xlab="k",col="grey",type="b")
lines(wf.prog.n1000.m005$d2,pch=2,col="indianred",type="b")
lines(wf.prog.n3000.m001$d2,pch=3,col="royalblue",type="b")
legend("bottomleft",legend=c(expression(paste("N=300; ",mu,"=0.01")),
                             expression(paste("N=300; ",mu,"=0.005")),
                             expression(paste("N=300; ",mu,"=0.001")))
       ,pch=c(20,2,3),col=c("darkgrey","indianred","royalblue"),bty="n")
```

### Progeny Distribution under object-mediated transmission

We explore whether and how the shape of the progeny distribution is affected by an unbiased object-mediated transmission with encoding and decoding errors. With a sufficiently large value of $T$ stochastic differences between runs become negligible, hence below we explore parameter combinations running each model one time over 10,000 generations.   


```{r parameters_progeny_exp1}
mu <- 0.01
N <- 300
warmup <- 10000
timesteps <- warmup + 10000
lambda <- c(0.1,1,5,10)
```

We first keep $N$ fixed to `r N[1]` and $\mu_{d}=\mu=$`r mu`, exploring different settings of $\lambda$:

```{r, execution_progeny_object_exp1}
decode.prog.varlambda <- vector("list",length=length(lambda))
encode.prog.varlambda <- vector("list",length=length(lambda))
legItems.progeny1=vector(length=length(lambda))

for (i in 1:length(lambda))
{
  decode.prog.varlambda[[i]] <-objTr(N=N,mu.e=0,mu.d=mu,
                                     lambda=lambda[i],warmup=10000,
                                     timesteps=20000,output="progeny")
  encode.prog.varlambda[[i]] <-objTr(N=N,mu.e=mu,mu.d=0,
                                     lambda=lambda[i],warmup=10000,
                                     timesteps=20000,output="progeny")  
  legItems.progeny1[i] = as.expression(bquote(paste(lambda,"=",.(lambda[i]),"; N=",.(N),sep="")))
}

legItems.progeny1=c(as.expression("Wright-Fisher"),legItems.progeny1)
```



```{r, parameters_progeny_exp2}
n.objects = 1000
N <- c(50,500,1000,2000)
lambda <- n.objects/N
```

We then look at instances where the number of objects produced is hold constant to approximately `r n.objects`, that is holding constant the product $\lambda \cdot N$:


```{r, execution_progeny_object_exp2}
encode.prog.fixobjects <- vector("list",length=length(N))
decode.prog.fixobjects <- vector("list",length=length(N))
legItems.progeny2=vector(length=length(N))


for (i in 1:length(N))
{
  decode.prog.fixobjects[[i]] <- objTr(N=N[i],mu.e=0,mu.d=mu,
                                       lambda=lambda[i],warmup=10000,
                                       timesteps=20000,output="progeny")
  encode.prog.fixobjects[[i]] <- objTr(N=N[i],mu.e=mu,mu.d=0,
                                       lambda=lambda[i],warmup=10000,
                                       timesteps=20000,output="progeny")  
  legItems.progeny2[i] = as.expression(bquote(paste(lambda,"=",.(lambda[i]),"; N=",.(N[i]),sep="")))

}
legItems.progeny2=c(as.expression("Wright-Fisher"),legItems.progeny2)
```

#### Results Decoding Error

```{r,plot_progeny_decode,fig.width=9,fig.height=5}
cc <- brewer.pal(length(lambda),'Set1')
par(mfrow=c(1,2))
plot(wf.prog.n300.m01$d2,type="n",lty=4,log="xy",ylab="Probability P(k) of number progeny >= k",xlab="k",col="darkgrey")
for (i in 1:length(decode.prog.varlambda))
{
  lines(decode.prog.varlambda[[i]]$d2, type="b",pch=20,col=cc[i])
}
lines(wf.prog.n300.m01$d2,lty=4,lwd=2)
legend("bottomleft",legend=legItems.progeny1,col=c(1,cc),pch=c(NA,rep(20,length(lambda))),lty=c(4,rep(NA,length(lambda))),lwd=c(2,rep(NA,length(lambda))),bty="n")

plot(wf.prog.n1000.m01$d2,type="n",lty=4,log="xy",ylab="Probability P(k) of number progeny >= k",xlab="k",col="darkgrey")
for (i in 1:length(decode.prog.fixobjects))
{
  lines(decode.prog.fixobjects[[i]]$d2, type="b",pch=20,col=cc[i])
}
lines(wf.prog.n1000.m01$d2,lty=4,lwd=2)
legend("bottomleft",legend=legItems.progeny2,col=c(1,cc),pch=c(NA,rep(20,length(N))),lty=c(4,rep(NA,length(N))),lwd=c(2,rep(NA,length(N))),bty="n")
```

The results on the left panel indicate that with low values of $\lambda$ the progeny distribution is very similar to the one expected from the Wright-Fisher model, with a power law section for lower values of $k$ followed by an exponential cut-off. When $\lambda\geq 5$, the exponential relationship starts at larger values of $k$, albeit with a slope similar to the one expected by the Wright-Fisher model. This is simply caused by the fact that when $\lambda>1$, obtaining $k=1$ instances of a variant becomes more difficult, as even if a mental template is associated to a single individual for one generation, the number of objects with such mental template is more likely to be larger than 1 (e.g. with $\lambda=5$ the probability of producing more than one copy of a variant is `r 1-ppois(1,lambda=5)`).

The results of righ panel considers different permutations of $\lambda$ and $N$ producing approximately the same number of objects for each generation. The progeny distributions confirm the effect of large values of $\lambda$ --- in this particular case the smallest observed case of $k$ is just `r decode.prog.fixobjects[[1]][[1]][1,1]` which is observed only `r decode.prog.fixobjects[[1]][[1]][1,2]` `r if(decode.prog.fixobjects[[1]][[1]][1,2]==1){"time"}` `r if(decode.prog.fixobjects[[1]][[1]][1,2]>1){"times"}`. The other parameter settings have all comparatively low values of $\lambda$, and hence the resulting distribution have a close match to the expectation under Wright-Fisher. 

#### Results Encoding Error

```{r,plot_progeny_encode,fig.width=9,fig.height=5}
cc <- brewer.pal(length(lambda),'Set1')
par(mfrow=c(1,2))
plot(wf.prog.n300.m01$d2,type="n",lty=4,log="xy",ylab="Probability P(k) of number progeny >= k",xlab="k",col="darkgrey")
for (i in 1:length(encode.prog.varlambda))
{
  lines(encode.prog.varlambda[[i]]$d2, type="b",pch=20,col=cc[i])
}
lines(wf.prog.n300.m01$d2,lty=4,lwd=2)
legend("bottomleft",legend=legItems.progeny1,col=c(1,cc),pch=c(NA,rep(20,length(lambda))),lty=c(4,rep(NA,length(lambda))),lwd=c(2,rep(NA,length(lambda))),bty="n")

plot(wf.prog.n1000.m01$d2,type="n",lty=4,log="xy",ylab="Probability P(k) of number progeny >= k",xlab="k",col="darkgrey")
for (i in 1:length(encode.prog.fixobjects))
{
  lines(encode.prog.fixobjects[[i]]$d2, type="b",pch=20,col=cc[i])
}
lines(wf.prog.n1000.m01$d2,lty=4,lwd=2)
legend("bottomleft",legend=legItems.progeny2,col=c(1,cc),pch=c(NA,rep(20,length(N))),lty=c(4,rep(NA,length(N))),lwd=c(2,rep(NA,length(N))),bty="n")
```

As for the decoding error scenario we first explore the effect of different $\lambda$ values on the shape of the progeny distribution, then combinations of $\lambda$ and $N$ yielding the same number of objects per generation (i.e. holding $\lambda \cdot N$ constant). The progeny distribution of the objected mediated transmission again deviates from the Wright Fisher model with increasing values of $\lambda$ (left panel). While the power-law and exponential cut-offs sections are retained in all cases, the former is disrupted by two processes. Firstly even with high values of $\lambda$, mutations occuring at the moment of production allows for a large number of variants appearing only once ($k=1$). However, at $k=2$ we observe a sudden drop in the number of variants. This is because an object can appear more than once if it has also a mental template in the population (i.e. there was a successful case of transmission), but with larger $\lambda$, lower values of $k$ are hard to achieve for the same reason as in the decoding error scenario. For example with $\lambda=5$, an object can appear in the assemblage twice ($k=2$) either because an individual produced two instances of its mental template in a given generation (a probability of `r dpois(2,5)`) but possessed the mental template only for that particular generation, or produced two instances in two generations (`r dpois(1,5)*dpois(1,5)`) before updating its mental template. Either option is comparatively rare, and hence the number of variants appearing only few times in the assemblage is rare with larger values of $\lambda$. Indeed the power-law section of the progeny distribution seems to approximately start from $k=\lambda$. 

The results of the experiment with a constant values for $\lambda \cdot N$ (right panel) shows again how low values of $\lambda$ yield a progeny distribution similar to Wright Fisher model, although with a higher drop in the proportion of variants after $k>1$. In the extreme case of $\lambda=20$ we do not observe any variants between $k=1$ and $k= `r encode.prog.fixobjects[[1]][[1]][2,1]`$, and after that the progeny distribution does follows a different trajectory compared to the Wright-Fisher model.

# References
1.Bentley, R.A., Lipo, C.P., Herzog, H.A., Hahn, M.W., 2007. [Regular rates of popular culture change reflect random copying](https://doi.org/10.1016/j.evolhumbehav.2006.10.002). Evolution and Human Behavior 28, 151–158.
2.Evans, T.S., Giometto, A., 2011. [Turnover Rate of Popularity Charts in Neutral Models](https://arxiv.org/abs/1105.4044v1). arXiv: physics:soc-ph.
3.Acerbi, A., Bentley, A.R., 2014. [Biases in cultural transmission shape the turnover of populat traits](http://dx.doi.org/10.1016/j.evolhumbehav.2014.02.003). Evolution and Human Behavior 35, 228–236.
4.Bentley, R.A., Hahn, M.W., Shennan, S.J., 2004. [Random drift and culture change](https://doi.org/10.1098/rspb.2004.2746). Procedings of The Royal Society B 271, 1443–1450.
5.O’Dwyer, J.P., Kandler, A., 2017. [Inferring processes of cultural transmission: the critical role of rare variants in distinguishing neutrality from novelty biases](https://doi.org/10.1098/rstb.2016.0426). Phil. Trans. R. Soc. B 372, 20160426. 
